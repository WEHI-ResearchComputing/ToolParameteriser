{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import csv\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import xml.etree.ElementTree as ET\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Job(ABC):\n",
    "    \"\"\"\n",
    "    The Job interface declares the operations that all concrete jobs\n",
    "    must implement.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def readBenchmarkingProfiles(self) -> list:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def createRepository(self) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def copyInputFiles(self) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def copySpecificInputFiles(self) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def createExecutableBatchFile(self) -> str:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def identifySpecificInputFiles(self) -> None:\n",
    "        pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Concrete Jobs provide various implementations of the Job interface.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class AbstractJob(Job):\n",
    "    def __init__(\n",
    "        self,\n",
    "        BenchmarkingCSVFile_path,\n",
    "        InputFiles_path,\n",
    "        storage_path,\n",
    "        Number_of_jobs_repetition,\n",
    "    ):\n",
    "        self.csvfile = BenchmarkingCSVFile_path\n",
    "        self.path = InputFiles_path\n",
    "        self.src = storage_path\n",
    "        self.dest = Number_of_jobs_repetition\n",
    "\n",
    "    def readBenchmarkingProfiles(self) -> list:\n",
    "        job_parameters = []\n",
    "\n",
    "        with open(self.csvfile, \"r\") as file:\n",
    "            csv_file = csv.DictReader(file)\n",
    "            for row in csv_file:\n",
    "                job_parameters.append(dict(row))\n",
    "\n",
    "        return job_parameters\n",
    "\n",
    "    def createRepository(self,running_job_path) -> None:\n",
    "        os.makedirs(running_job_path)\n",
    "\n",
    "    def copyInputFiles(self, src, dest) -> None:\n",
    "        try:\n",
    "            shutil.copytree(src, dest, dirs_exist_ok=True)\n",
    "        except NotADirectoryError:\n",
    "            shutil.copy(src, dest)\n",
    "\n",
    "    def copySpecificInputFiles(self) -> str:\n",
    "        return \"{Result of the AbstractJob: copySpecificInputFiles}\"\n",
    "\n",
    "    def createExecutableBatchFile(self) -> str:\n",
    "        return \"{Result of the AbstractJob: createExecutableBatchFile}\"\n",
    "\n",
    "    def identifySpecificInputFiles(self) -> str:\n",
    "        return \"{Result of the AbstractJob: identifySpecificInputFiles}\"\n",
    "\n",
    "\n",
    "class MaxQuantJob(AbstractJob):\n",
    "    def __init__(\n",
    "        self,\n",
    "        BenchmarkingCSVFile_path,\n",
    "        InputFiles_path,\n",
    "        storage_path,\n",
    "        Number_of_jobs_repetition,\n",
    "\n",
    "    ):\n",
    "        super().__init__(\n",
    "            BenchmarkingCSVFile_path,\n",
    "            InputFiles_path,\n",
    "            storage_path,\n",
    "            Number_of_jobs_repetition,\n",
    "        )\n",
    "        # TODO: Variables needs to be initialize \n",
    "        # self.sample_files = sample_files\n",
    "        # self.xml_file_path = xml_file_path\n",
    "        # self.numthreads = numthreads\n",
    "\n",
    "    def createExecutableBatchFile(self, job_parameters, path, ExecutionID) -> None:\n",
    "        with open(f\"{path}{job_parameters['job-name']}_batch.sh\", \"w+\") as fb:\n",
    "            fb.writelines(\"#!/bin/bash\\n\")\n",
    "            fb.writelines(f\"#SBATCH -p {job_parameters['partition']}\\n\")\n",
    "            fb.writelines(f\"#SBATCH --qos=regular_partitiontimelimit\\n\")\n",
    "            fb.writelines(f\"#SBATCH --job-name={job_parameters['job-name']}\\n\")\n",
    "            fb.writelines(f\"#SBATCH --ntasks=1\\n\")\n",
    "            fb.writelines(f\"#SBATCH --time={job_parameters['timelimit']}\\n\")\n",
    "            fb.writelines(\n",
    "                f\"#SBATCH --cpus-per-task={job_parameters['cpus-per-task']}\\n\"\n",
    "            )\n",
    "            fb.writelines(f\"#SBATCH --mem={job_parameters['mem']}G\\n\")\n",
    "            fb.writelines(f\"#SBATCH --output={path}slurm-%j.out\\n\")\n",
    "            fb.writelines(f\"#SBATCH --mail-type=ALL,ARRAY_TASKS\\n\")\n",
    "            fb.writelines(f\"#SBATCH --mail-user=romano.h@wehi.edu.au\\n\")\n",
    "\n",
    "            fb.writelines(f\"module load MaxQuant/2.0.2.0\\n\")\n",
    "            fb.writelines(f\"module load python/3.8.8\\n\")\n",
    "\n",
    "            fb.writelines(\n",
    "                f\"MaxQuant {path}mqpar.xml --changeFolder {path}mqpar.post.xml {path} {path}\\n\"\n",
    "            )\n",
    "\n",
    "            fb.writelines(f\"MaxQuant {path}mqpar.post.xml\\n\")\n",
    "\n",
    "            fb.writelines(\n",
    "                f\"find {path} -maxdepth 1 -mindepth 1 -type f -not -regex '.*\\.\\(fasta\\|xml\\|out\\|raw\\|sh\\)' -delete\\n\"\n",
    "            )\n",
    "            fb.writelines(\n",
    "                f\"find {path} -maxdepth 1 -mindepth 1 -type d -not -regex '.*\\.\\(d\\)' -exec rm -rf '{{}}' \\;\\n\"\n",
    "            )\n",
    "\n",
    "            fb.writelines(\n",
    "                f'echo \"\"$SLURM_ARRAY_JOB_ID\",\"$SLURM_ARRAY_TASK_ID\"\",{job_parameters[\"partition\"]},{job_parameters[\"type\"]},{job_parameters[\"job-name\"]},{job_parameters[\"NumFiles\"]},{job_parameters[\"cpus-per-task\"]},{job_parameters[\"mem\"]},{job_parameters[\"threads\"]},{job_parameters[\"timelimit\"]} >> jobs_executed_{ExecutionID}.txt\\n'\n",
    "            )\n",
    "\n",
    "        os.system(f\"sbatch {path}{job_parameters['job-name']}_batch.sh\")\n",
    "\n",
    "    def updateXmlFile(self, sample_files, xml_file_path, numthreads) -> None:\n",
    "        tree = ET.parse(xml_file_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        for filepath_tag in root.findall(\"filePaths/string\"):\n",
    "            root.findall(\"filePaths\")[0].remove(filepath_tag)\n",
    "\n",
    "        for sample_file in sample_files:\n",
    "            new_path = ET.Element(\"string\")\n",
    "            new_path.text = sample_file\n",
    "            root.findall(\"filePaths\")[0].append(new_path)\n",
    "\n",
    "        # <useDotNetCore>True</useDotNetCore>\n",
    "        root.findall(\"useDotNetCore\")[0].text = \"True\"\n",
    "        # <numThreads>8</numThreads>\n",
    "        root.findall(\"numThreads\")[0].text = str(numthreads)\n",
    "\n",
    "        outputfile = xml_file_path\n",
    "        tree.write(outputfile)\n",
    "\n",
    "\n",
    "class DiaNNJob(AbstractJob):\n",
    "    def __init__(\n",
    "        self,\n",
    "        BenchmarkingCSVFile_path,\n",
    "        InputFiles_path,\n",
    "        storage_path,\n",
    "        Number_of_jobs_repetition,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            BenchmarkingCSVFile_path,\n",
    "            InputFiles_path,\n",
    "            storage_path,\n",
    "            Number_of_jobs_repetition,\n",
    "        )\n",
    "    \n",
    "    def identifySpecificInputFiles(self) -> dict:\n",
    "        # Extract the list of Input filenames: .Fasta, .tsv and .d\n",
    "        original_files = glob.glob(self.path + \"*.d\", recursive=False)\n",
    "\n",
    "        # Create a Dictionary to store Input Files Orderly\n",
    "        DiaNNSpecificInputFiles = {}\n",
    "        DiaNNSpecificInputFiles[\"original_files\"] = original_files\n",
    "        DiaNNSpecificInputFiles[\"fasta_file\"] = glob.glob(\n",
    "            self.path + \"*.fasta\", recursive=False\n",
    "        )[0]\n",
    "        \n",
    "\n",
    "        if glob.glob(self.path + \"*.tsv\", recursive=False):\n",
    "            DiaNNSpecificInputFiles[\"tsv_file\"] = glob.glob(\n",
    "                self.path + \"*.tsv\", recursive=False\n",
    "            )[0]\n",
    "        else:\n",
    "            DiaNNSpecificInputFiles[\"tsv_file\"] = None\n",
    "            \n",
    "        return DiaNNSpecificInputFiles\n",
    "\n",
    "    def copySpecificInputFiles(self,specificInputFiles, running_job_path) -> None:\n",
    "        # Copy Fasta & XML File\n",
    "        self.copyInputFiles(specificInputFiles[\"fasta_file\"], running_job_path)\n",
    "                \n",
    "        if specificInputFiles[\"tsv_file\"]:\n",
    "            self.copyInputFiles(specificInputFiles[\"tsv_file\"], running_job_path)\n",
    "\n",
    "    def createExecutableBatchFile(\n",
    "        self, job_parameters, path, specificInputFiles, ExecutionID\n",
    "    ) -> None:\n",
    "\n",
    "        os.system(f'(cd {path} ; DIANN_RUN_TYPE=\"\"{job_parameters[\"type\"]}\"\" DIANN_LIB=\"\"{specificInputFiles[\"tsv_file\"]}\"\" DIANN_TIME=\"\"{job_parameters[\"timelimit\"]}\"\"  DIANN_CPUS=\"\"{job_parameters[\"cpus-per-task\"]}\"\" DIANN_MEM=\"\"{job_parameters[\"mem\"]}G\"\" DIANN_THREADS=\"\"{job_parameters[\"threads\"]} DIANN_OUTPUT_PATH=\"\"{path}/output\"\" /stornext/System/data/apps/rc-tools/rc-tools-1.0/bin/tools/DiaNN/createdianncmd.sh)')\n",
    "\n",
    "        with open(f\"{path}diann.slurm\", \"a\") as fb:\n",
    "            fb.writelines(f'echo \"\"$SLURM_JOB_ID\"\",{job_parameters[\"partition\"]},{job_parameters[\"type\"]},{job_parameters[\"job-name\"]},{job_parameters[\"NumFiles\"]},{job_parameters[\"cpus-per-task\"]},{job_parameters[\"mem\"]},{job_parameters[\"threads\"]},{job_parameters[\"timelimit\"]} >> jobs_executed_{ExecutionID}.txt\\n')\n",
    "\n",
    "        os.system(f\"sbatch {path}diann.slurm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BenchmarkingToolCreator(ABC):\n",
    "    \"\"\"\n",
    "    The BenchmarkingToolCreator class declares the factory method that is supposed to return an\n",
    "    object of a Job class. The BenchmarkingToolCreator's subclasses usually provide the\n",
    "    implementation of this method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        BenchmarkingCSVFile_path,\n",
    "        InputFiles_path,\n",
    "        storage_path,\n",
    "        Number_of_jobs_repetition,\n",
    "    ):\n",
    "        self.BenchmarkingCSVFile_path = BenchmarkingCSVFile_path\n",
    "        self.InputFiles_path = InputFiles_path\n",
    "        self.storage_path = storage_path\n",
    "        self.Number_of_jobs_repetition = Number_of_jobs_repetition\n",
    "\n",
    "    @abstractmethod\n",
    "    def factory_method_create_job(self):\n",
    "        \"\"\"\n",
    "        Note that the BenchmarkingToolCreator may also provide some default implementation of\n",
    "        the factory method.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def runBenchmarking(self) -> str:\n",
    "        \"\"\"\n",
    "        Also note that, despite its name, the BenchmarkingToolCreator's primary responsibility\n",
    "        is not creating jobs. Usually, it contains some core business logic\n",
    "        that relies on Job objects, returned by the factory method.\n",
    "        Subclasses can indirectly change that business logic by overriding the\n",
    "        factory method and returning a different type of job from it.\n",
    "        \"\"\"\n",
    "\n",
    "        # Call the factory method to create a Job object.\n",
    "        job = self.factory_method_create_job()\n",
    "        # Now, use the job.\n",
    "\n",
    "        # TODO: Inside of this method will be our core business logic\n",
    "        \n",
    "        # current date and time\n",
    "        now = datetime.now()\n",
    "\n",
    "        # ID to identify each Benchmarking executed\n",
    "        ExecutionID = now.strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "        # Storing job_parameters of CSV file\n",
    "        job_parameters = job.readBenchmarkingProfiles()\n",
    "\n",
    "        # Specific files Identification (.tsv, .d, .xml, .fasta)\n",
    "        specificInputFiles = job.identifySpecificInputFiles()\n",
    "\n",
    "        # Let's run the job according to the number of repetition\n",
    "        for parameters in job_parameters:\n",
    "\n",
    "            for _ in range(0, self.Number_of_jobs_repetition):\n",
    "                now = datetime.now()  # current date and time\n",
    "                JobExecutionID = now.strftime(\"%Y%m%d%H%M%S\")\n",
    "                running_job_path = f\"{self.storage_path}repo-{parameters['job-name']}-{JobExecutionID}/\"\n",
    "\n",
    "                job.createRepository(running_job_path)\n",
    "\n",
    "                # Copy samples files k = number of input files to randomly select\n",
    "                sample_files = random.sample(specificInputFiles[\"original_files\"], k=int(parameters[\"NumFiles\"]))\n",
    "                specificInputFiles[\"sample_files\"] = sample_files\n",
    "\n",
    "                for sample_file_path in specificInputFiles[\"sample_files\"]:\n",
    "                    name_of_folder = sample_file_path.split(\"/\")[-1]\n",
    "                    job.copyInputFiles(sample_file_path, running_job_path + name_of_folder)\n",
    "\n",
    "                # Copy ONLY specific input files such (.tsv, .xml, .fasta)\n",
    "                job.copySpecificInputFiles(specificInputFiles, running_job_path)\n",
    "        \n",
    "                # Create and Execute the SBatch File\n",
    "\n",
    "                job.createExecutableBatchFile(\n",
    "                    parameters,\n",
    "                    running_job_path,\n",
    "                    specificInputFiles,\n",
    "                    ExecutionID,\n",
    "                )\n",
    "\n",
    "        \n",
    "        # result = f\"BenchmarkingToolCreator: The same creator's code has just worked with {job.readBenchmarkingProfiles()}\"\n",
    "        result = \"BenchMe has finished running\"\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Concrete Creators override the factory method in order to change the resulting\n",
    "product's type.\n",
    "\"\"\"\n",
    "\n",
    "class MQBenchmarkingTool(BenchmarkingToolCreator):\n",
    "    \"\"\"\n",
    "    Note that the signature of the method still uses the abstract job type,\n",
    "    even though the concrete job is actually returned from the method. This\n",
    "    way the BenchmarkingToolCreator can stay independent of concrete job classes.\n",
    "    \"\"\"\n",
    "    def __init__(self,BenchmarkingCSVFile_path, InputFiles_path, storage_path, Number_of_jobs_repetition,):\n",
    "        super().__init__(BenchmarkingCSVFile_path, InputFiles_path, storage_path, Number_of_jobs_repetition)\n",
    "\n",
    "        # TODO: Variables needs to be initialize\n",
    "        # Extract the list of Input filenames: .Fasta, .XML and .d\n",
    "        # original_files = glob.glob(InputFiles_path + \"*.d\", recursive=False)\n",
    "\n",
    "        # Create a Dictionary to store Input Files Orderly\n",
    "        # MaxQuantInputFiles = {}\n",
    "        # MaxQuantInputFiles[\"fasta_file\"] = glob.glob(InputFiles_path + \"*.fasta\", recursive=False)[0]\n",
    "        # MaxQuantInputFiles[\"xml_file\"] = glob.glob(InputFiles_path + \"*.xml\", recursive=False)[0]\n",
    "\n",
    "        # self.sample_files = sample_files\n",
    "        # self.xml_file_path = xml_file_path\n",
    "        # self.numthreads = numthreads\n",
    "        \n",
    "    def factory_method_create_job(self) -> Job:\n",
    "        return MaxQuantJob(self.BenchmarkingCSVFile_path,self.InputFiles_path,self.storage_path,self.Number_of_jobs_repetition )\n",
    "\n",
    "\n",
    "class DiaNNBenchmarkingTool(BenchmarkingToolCreator):\n",
    "    def __init__(self,BenchmarkingCSVFile_path, InputFiles_path, storage_path, Number_of_jobs_repetition):\n",
    "        super().__init__(BenchmarkingCSVFile_path, InputFiles_path, storage_path, Number_of_jobs_repetition)\n",
    "        \n",
    "    def factory_method_create_job(self) -> Job:\n",
    "        return DiaNNJob(self.BenchmarkingCSVFile_path,self.InputFiles_path,self.storage_path,self.Number_of_jobs_repetition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add this function to your diagram\n",
    "def client_code(creator: BenchmarkingToolCreator) -> None:\n",
    "    \"\"\"\n",
    "    The client code works with an instance of a concrete creator, albeit through\n",
    "    its base interface. As long as the client keeps working with the creator via\n",
    "    the base interface, you can pass it any creator's subclass.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\n",
    "            f\"Client: I'm not aware of the creator's class, but it still works.\\n\"\n",
    "            f\"{creator.runBenchmarking()}\",\n",
    "            end=\"\",\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welcoming message\n",
    "BenchTool = input(\"Thanks for using BenchMe! Please select '1' if you want benchmark MQ or '2' for DiaNN\")\n",
    "\n",
    "InputFiles_path = input(\"Absolute path of Input DiaNN input files (.d, .fasta, .tsv). I.E: '/stornext/HPCScratch/home/romano.h/Software-Projects/Local-Repositories/DiaNN/DiaNNFiles-Dataset/'\")\n",
    "BenchmarkingCSVFile_path = input(\"Name of CSV file with DiaNN Profiles for benchmarking. I.E: 'benchmarking-profiles.csv'\")\n",
    "storage_path = input(\"Absolute path of storage directory to save outputs. I.E: '/vast/scratch/users/romano.h/DiaNNBenchmarking/'\")\n",
    "Number_of_jobs_repetition = int(input(\"Number of times to run each benchmarking profile job. Default: 5 times\"))\n",
    "\n",
    "if BenchTool == '1':\n",
    "    print(\"App: Launched with the MQBenchmarkingTool.\")\n",
    "    client_code(MQBenchmarkingTool(BenchmarkingCSVFile_path, InputFiles_path, storage_path, Number_of_jobs_repetition))\n",
    "elif BenchTool == '2':\n",
    "    print(\"App: Launched with the DiaNNBenchmarkingTool.\")\n",
    "    client_code(DiaNNBenchmarkingTool(BenchmarkingCSVFile_path, InputFiles_path, storage_path, Number_of_jobs_repetition))\n",
    "else:\n",
    "    print(\"I am sorry, the selected option is invalid.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This will need to be added/modified when migrating out of Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add this function to your diagram\n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    The client code works with an instance of a concrete creator, albeit through\n",
    "    its base interface. As long as the client keeps working with the creator via\n",
    "    the base interface, you can pass it any creator's subclass.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"App: Launched with the MQBenchmarkingTool.\")\n",
    "    client_code(MQBenchmarkingTool())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"App: Launched with the DiaNNBenchmarkingTool.\")\n",
    "    client_code(DiaNNBenchmarkingTool())\n",
    "\n",
    "    def client_code(creator: BenchmarkingToolCreator):\n",
    "\n",
    "        print(\n",
    "            f\"Client: I'm not aware of the creator's class, but it still works.\\n\"\n",
    "            f\"{creator.runBenchmarking()}\",\n",
    "            end=\"\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string(\"files\", None, \"Absolute path of Input DiaNN input files (.d, .fasta, .tsv). I.E: '/stornext/HPCScratch/home/romano.h/Software-Projects/Local-Repositories/DiaNN/DiaNNFiles-Dataset/'\")\n",
    "flags.DEFINE_string(\"profiles\", None, \"Name of CSV file with DiaNN Profiles for benchmarking. I.E: 'benchmarking-profiles.csv'\")\n",
    "flags.DEFINE_string(\"storage\", None, \"Absolute path of storage directory to save outputs. I.E: '/vast/scratch/users/romano.h/DiaNNBenchmarking/'\")\n",
    "flags.DEFINE_integer(\"repeat\", 5,\"Number of times to run each benchmarking profile job. Default: 5 times\")\n",
    "\n",
    "# Required flag.\n",
    "flags.mark_flag_as_required(\"files\")\n",
    "flags.mark_flag_as_required(\"profiles\")\n",
    "flags.mark_flag_as_required(\"storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    app.run(main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1cd49daa982e2d0df3520171ed1bc31370236c716747a62405fce76878819793"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('testexample-xrcCy3tl': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
